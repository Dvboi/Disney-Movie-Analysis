{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Info Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_it(url,title):\n",
    "    final_url = \"https://en.wikipedia.org\"+url   # take url for respective film\n",
    "    r = requests.get(final_url)\n",
    "    soup = bs(r.content)\n",
    "\n",
    "    table = soup.find(\"table\",attrs={\"class\":\"infobox vevent\"})  # get the table for that mov\n",
    "    try:    # to catch one extreme edge case of duplicate tag\n",
    "        headers = table.find_all(\"th\",attrs = {\"scope\" : \"row\"})    # get the titles for rows\n",
    "    except:\n",
    "        print(soup.prettify())\n",
    "    d = {}   # to store data\n",
    "    d['Title'] = title\n",
    "    rows = table.find_all(\"tr\")    # get the rows of the info_table for a mov\n",
    "    for row in rows:     # for every row get the values for the respective titles\n",
    "        header = row.find(\"th\",attrs = {\"scope\" : \"row\"})\n",
    "        if header:\n",
    "            key = header.get_text(\" \").strip() # \" \" is the string to be used to join the bits of text together\n",
    "            value = row.find(\"td\").get_text().strip().replace(\"\\xa0\",\" \").split('\\n')\n",
    "            d[key] = value\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the info box for all movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_Walt_Disney_Pictures_films\"\n",
    "r = requests.get(url)\n",
    "soup = bs(r.content)\n",
    "# to get all the tables of wiki page, [:-1] except the lst table which contains a few upcoming movies\n",
    "# and obviously some important data will be missing there.\n",
    "tables = soup.find_all(\"table\",attrs = {\"class\" : \"wikitable sortable\"})[:-1]\n",
    "lst = []  # to store info_table of every mov\n",
    "for table in tables:  #for every mov\n",
    "    films = table.find_all(\"i\")   # get tag of all movies from table and store in films\n",
    "    for i in films:   # for each film tag\n",
    "        if i.a:  # if film/mov link exists\n",
    "            title = i.a.get_text(strip=True) # get title\n",
    "            link = i.a[\"href\"] # get the film/mov link\n",
    "#             print(link)  # to debug\n",
    "            if link==\"/wiki/True-Life_Adventures\":   # for one extreme edge case where the tags were exactly same for 2 different links    \n",
    "#                 print(\"Invalid,passing onto next...\") # to debug\n",
    "                continue\n",
    "            d = get_it(link,title)   # get data for that mov using the link stored earlier\n",
    "#             print(link) # to debug\n",
    "#             print('*'*100) # to debug\n",
    "            lst.append(d)  # store data of every mov in lst\n",
    "        else:\n",
    "            link = \"No Link\"  # if no link exists for that mov, another edge case and i don't store it\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"wiki_movs.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "#     json.dump(lst,f,ensure_ascii=False,indent=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(title):\n",
    "    with open(title,encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach Scores With Api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del lst\n",
    "mov_info = load_json(\"wiki_movs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "def get_scores(title):\n",
    "    base_url = \"http://www.omdbapi.com/?\"\n",
    "    parameters = {\"apikey\":\"70b3cc9f\",\"t\":title}\n",
    "    end_url = urllib.parse.urlencode(parameters)\n",
    "    final_url = base_url + end_url\n",
    "    return requests.get(final_url).json()\n",
    "\n",
    "def get_rts(temp):\n",
    "    r = temp.get('Ratings',[])\n",
    "    for ele in r:\n",
    "        if ele['Source'] == 'Rotten Tomatoes':\n",
    "            return ele['Value']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mov in mov_info:\n",
    "    title = mov['Title']\n",
    "    temp = get_scores(title)\n",
    "    mov['Metascore'] = temp.get('Metascore',None)\n",
    "    mov['imdbRating'] = temp.get('imdbRating',None)\n",
    "    mov['Rotten_Tomatoes'] = get_rts(temp)\n",
    "    mov['Genre'] = temp.get('Genre',None)\n",
    "    mov['Rated'] = temp.get('Rated',None)\n",
    "    mov['BoxOffice_Api'] = temp.get('BoxOffice',None)\n",
    "    mov['Type'] = temp.get('Type',None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving to Json\n",
    "\n",
    "# import json\n",
    "# with open(\"Final_data.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "#     json.dump(mov_info,f,ensure_ascii=False,indent=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to CSV\n",
    "\n",
    "# df = pd.DataFrame(mov_info)\n",
    "# df.to_csv('Final_Data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
